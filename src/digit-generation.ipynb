{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Digit Generation\n",
    "\n",
    "Train a generative adversarial network (GAN) to generate realistic handwritten digits similar to those in the MNIST dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries for PyTorch and data visualization\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.init as init\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Setting a random seed for reproducibility\n",
    "random_seed = 1\n",
    "torch.backends.cudnn.enabled = False  # Disabling CuDNN for deterministic results\n",
    "_ = torch.manual_seed(random_seed)  # Setting manual seed for random number generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create useful folders\n",
    "folders = [\"../data\", \"../results/\", \"../results/digit-generation/\"]\n",
    "for f in folders:\n",
    "    if not os.path.exists(f):\n",
    "        os.mkdir(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining batch sizes for training and testing data\n",
    "batch_size_train = 64\n",
    "batch_size_test = 1000\n",
    "\n",
    "# Defining data transformations, including converting images to tensors and normalizing pixel values\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]\n",
    ")\n",
    "\n",
    "# Loading MNIST dataset for training and testing\n",
    "# For training data, setting train=True, downloading if not available, and applying transformations\n",
    "train_dataset = datasets.MNIST(\"../data\", train=True, download=True, transform=transform)\n",
    "\n",
    "# For testing data, setting train=False, not downloading, and applying transformations\n",
    "train_dataset = datasets.MNIST(\"../data\", train=False, transform=transform)\n",
    "\n",
    "# Creating data loaders for efficient batch processing during training and testing\n",
    "# For training data, using DataLoader with specified batch size and shuffling the data\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=batch_size_train, shuffle=True\n",
    ")\n",
    "\n",
    "# For testing data, using DataLoader with specified batch size and shuffling the data\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=batch_size_test, shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting one batch of examples from the test_loader\n",
    "batch_idx, (example_data, example_targets) = next(enumerate(test_loader))\n",
    "\n",
    "# Printing the shape of the example_data tensor\n",
    "print(\"Shape of example_data:\", example_data.shape)\n",
    "\n",
    "# Plotting the first images along with their labels\n",
    "fig, axes = plt.subplots(5, 5, figsize=(10, 10))\n",
    "\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    ax.imshow(example_data[i][0], cmap=\"gray\", interpolation=\"none\")\n",
    "    ax.set_title(\"{}\".format(example_targets[i]))\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, nz):\n",
    "        super(Generator, self).__init__()\n",
    "        self.nz = nz\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Linear(self.nz, 256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(256, 512),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(512, 1024),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(1024, 784),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.main(x).view(-1, 1, 28, 28)\n",
    "    \n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.n_input = 784\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Linear(self.n_input, 1024),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, 1),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 784)\n",
    "        return self.main(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_dim = 100\n",
    "G = Generator(z_dim)\n",
    "D = Discriminator()\n",
    "\n",
    "print(G)\n",
    "print(D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import shutil\n",
    "\n",
    "# Path to the log directory\n",
    "log_dir = \"./runs/digit-generation/generator\"\n",
    "\n",
    "# Remove the previous log directory (if it exists)\n",
    "shutil.rmtree(log_dir, ignore_errors=True)\n",
    "\n",
    "# Initialize TensorBoard writer\n",
    "writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "# Visualize the network architecture\n",
    "dummy_input = torch.rand(1, z_dim)  # Create a dummy input tensor\n",
    "writer.add_graph(G, dummy_input)\n",
    "\n",
    "# Path to the log directory\n",
    "log_dir = \"./runs/digit-generation/discriminator\"\n",
    "\n",
    "# Remove the previous log directory (if it exists)\n",
    "shutil.rmtree(log_dir, ignore_errors=True)\n",
    "\n",
    "# Initialize TensorBoard writer\n",
    "writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "# Visualize the network architecture\n",
    "dummy_input = torch.rand(1, 1, 28, 28)  # Create a dummy input tensor\n",
    "writer.add_graph(D, dummy_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize BCELoss function\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# Setup Adam optimizers for both G and D\n",
    "D_optimizer = optim.Adam(D.parameters(), lr=0.0002)\n",
    "G_optimizer = optim.Adam(G.parameters(), lr=0.0002)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_train(batch_size):\n",
    "    # Reset gradients\n",
    "    G_optimizer.zero_grad()\n",
    "\n",
    "    # Generate random noise\n",
    "    z = torch.randn(batch_size, z_dim)\n",
    "\n",
    "    # Generate fake data with the generator\n",
    "    fake_data = G(z)\n",
    "\n",
    "    # Labels for the generated data\n",
    "    fake_labels = torch.ones(batch_size, 1)\n",
    "\n",
    "    # Pass the generated data through the discriminator\n",
    "    D_output = D(fake_data)\n",
    "\n",
    "    # Calculate the generator loss\n",
    "    G_loss = criterion(D_output, fake_labels)\n",
    "\n",
    "    # Backpropagation and optimization\n",
    "    G_loss.backward()\n",
    "    G_optimizer.step()\n",
    "\n",
    "    return G_loss.item()\n",
    "\n",
    "\n",
    "def discriminator_train(real_data, batch_size):\n",
    "    # Reset gradients\n",
    "    D_optimizer.zero_grad()\n",
    "\n",
    "    # Labels for the real and fake data\n",
    "    real_labels = torch.ones(batch_size, 1)\n",
    "    fake_data = G(torch.randn(batch_size, z_dim))\n",
    "    fake_labels = torch.zeros(batch_size, 1)\n",
    "\n",
    "    # Pass the real and fake data through the discriminator\n",
    "    D_real_output = D(real_data)\n",
    "    D_fake_output = D(fake_data.detach())  # Detach fake_data\n",
    "\n",
    "    # Calculate the discriminator loss for real and fake data\n",
    "    D_real_loss = criterion(D_real_output, real_labels)\n",
    "    D_fake_loss = criterion(D_fake_output, fake_labels)\n",
    "\n",
    "    # Total discriminator loss\n",
    "    D_loss = D_real_loss + D_fake_loss\n",
    "\n",
    "    # Backpropagation and optimization\n",
    "    D_loss.backward()\n",
    "    D_optimizer.step()\n",
    "\n",
    "    return D_loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nombre d'époques\n",
    "num_epochs = 20\n",
    "\n",
    "# Boucle d'entraînement\n",
    "D_losses, G_losses = [], []\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    # Boucle pour entraîner le générateur et le discriminateur à chaque batch\n",
    "    for i, (real_data, _) in enumerate(train_loader):\n",
    "        # Taille du batch\n",
    "        batch_size = real_data.size(0)\n",
    "\n",
    "        # Entraîner le générateur\n",
    "        generator_loss = generator_train(batch_size)\n",
    "        G_losses.append(generator_loss)\n",
    "\n",
    "        # Entraîner le discriminateur\n",
    "        discriminator_loss = discriminator_train(\n",
    "            real_data.view(-1, 784), batch_size\n",
    "        )\n",
    "        D_losses.append(discriminator_loss)\n",
    "\n",
    "        # Affichage des informations d'entraînement\n",
    "        if i % 100 == 0:\n",
    "            print(\n",
    "                f\"Epoch [{epoch}/{num_epochs}], Batch [{i}/{len(train_loader)}], generator_loss: {torch.mean(torch.FloatTensor(G_losses)):.4f}, discriminator_loss: {torch.mean(torch.FloatTensor(D_losses)):.4f}\"\n",
    "            )\n",
    "    print(\n",
    "        f\"\\n\\t\\t===> Epoch {epoch}, G-Loss: {torch.mean(torch.FloatTensor(G_losses)):.4f}, D-Loss: {torch.mean(torch.FloatTensor(D_losses)):.4f}\\n\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définir le nombre de colonnes et de lignes pour afficher les images\n",
    "n_rows = 5\n",
    "n_cols = 5\n",
    "\n",
    "# Créer une nouvelle figure avec une grille de sous-graphiques\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(10, 10))\n",
    "\n",
    "# Désactiver le calcul des gradients\n",
    "with torch.no_grad():\n",
    "    # Générer des images avec le générateur\n",
    "    test_z = torch.randn(n_rows * n_cols, z_dim)\n",
    "    generated = G(test_z).detach()\n",
    "\n",
    "    # Itérer sur chaque image générée et l'afficher dans un sous-graphique\n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        ax.imshow(generated[i].view(28, 28), cmap='gray')\n",
    "        ax.axis('off')\n",
    "\n",
    "# Afficher la figure\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
